{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLTKrNadVyzU",
        "outputId": "7a5d0e96-acd4-4011-c942-c7b73c8837f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-6099db43748c>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Phish URL'] = df['Phish URL'].str.replace(r'(added).*$', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated data saved as updated_example.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read the data from the CSV file\n",
        "df = pd.read_csv('/content/table_values_invalid (4).csv')\n",
        "\n",
        "# replace URLs that have \"added to\" until the end of the URL with a space\n",
        "df['Phish URL'] = df['Phish URL'].str.replace(r'(added).*$', '')\n",
        "\n",
        "# save the updated data as a new CSV file\n",
        "df.to_csv('updated_example_invalid.csv', index=False)\n",
        "\n",
        "# print a message to confirm that the file has been saved\n",
        "print('Updated data saved as updated_example.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/table_values_invalid (4).csv\")"
      ],
      "metadata": {
        "id": "cJ5Esc1ZqZaj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = data['ID'].values"
      ],
      "metadata": {
        "id": "7xHaYqRnrFHy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "# The base URL of the webpage to scrape\n",
        "base_url = 'https://phishtank.org/phish_detail.php?phish_id={}'\n",
        "\n",
        "# Open a CSV file for writing\n",
        "with open('real_whitelist.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['ID', 'Phish URL'])\n",
        "    g=1\n",
        "    for i in arr: \n",
        "        g=g+1\n",
        "        url = base_url.format(i)\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Create a BeautifulSoup object to parse the HTML content of the response\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        span_tag = soup.find('span', attrs={'style': 'word-wrap:break-word;'}).find('b')\n",
        "        value = span_tag.text\n",
        "        if(g%15==0):\n",
        "            time.sleep(5)\n",
        "        # Extract the values from each row and add them to a list of lists\n",
        "        table_values = [[i,value]]\n",
        "        # Write the table values to the CSV file, with each row in a separate row\n",
        "        writer.writerows(table_values)"
      ],
      "metadata": {
        "id": "0_nwcimtqOV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSw0PvSZrNP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}